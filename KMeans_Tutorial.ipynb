{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-Means Tutorial\n",
    "===\n",
    "\n",
    "K-means is an example of unsupervised learning through clustering. It tries to separate unlabeled data into clusters with equal variance. In two dimensions, this can be visualized as grouping data using circular areas of equal radius.\n",
    "\n",
    "There are three steps training a K-means classifier: \n",
    "\n",
    "1. Pick how many groups you want it to use and (randomly) assign a starting centroid (center point) to each cluster.\n",
    "2. Assign each data point to the group with the closest centroid.\n",
    "3. Find the mean value of each feature (the middle point of the cluster) for all the points assinged to each cluster. This is the new centroid for that cluster.\n",
    "\n",
    "Steps 2 and 3 repeat until the cluster centroids do not move significantly.\n",
    "\n",
    "Scikit-learn provides more information on the K-means classifier function [KMeans](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html). They also have an examples of using K-means to [classify handwritten numbers](http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_digits.html#example-cluster-plot-kmeans-digits-py).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup\n",
    "===\n",
    "Tell matplotlib to print figures in the notebook. Then import numpy (for numerical data), pyplot (for plotting figures) ListedColormap (for plotting colors), kmeans (for the scikit-learn kmeans algorithm) and datasets (to download the iris dataset from scikit-learn).\n",
    "\n",
    "Also create the color maps to use to color the plotted data, and \"labelList\", which is a list of colored rectangles to use in plotted legends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print figures in the notebook\n",
    "%matplotlib inline \n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn import datasets # Import the dataset from scikit-learn\n",
    "from sklearn.cluster import KMeans # Import the KMeans classifier\n",
    "\n",
    "# Import patch for drawing rectangles in the legend\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "# Create color maps\n",
    "cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\n",
    "cmap_bg = ListedColormap(['#333333', '#666666', '#999999'])\n",
    "\n",
    "# Create a legend for the colors, using rectangles for the corresponding colormap colors\n",
    "labelList = []\n",
    "for color in cmap_bold.colors:\n",
    "    labelList.append(Rectangle((0, 0), 1, 1, fc=color))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the dataset\n",
    "===\n",
    "Import the dataset and store it to a variable called iris. Scikit-learn's explanation of the dataset is [here](http://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html). This dataset is similar to a python dictionary, with the keys: ['DESCR', 'target_names', 'target', 'data', 'feature_names']\n",
    "\n",
    "The data features are stored in iris.data, where each row is an example from a single flower, and each column is a single feature. The feature names are stored in iris.feature_names. Labels are stored as the numbers 0, 1, or 2 in iris.target, and the names of these labels are in iris.target_names.\n",
    "\n",
    "The dataset consists of measurements made on 50 examples from each of three different species of iris flowers (Setosa, Versicolour, and Virginica). Each example has four features (or measurements): [sepal](http://en.wikipedia.org/wiki/Sepal) length, sepal width, [petal](http://en.wikipedia.org/wiki/Petal) length, and petal width. All measurements are in cm.\n",
    "\n",
    "Below, we load the labels into y, the corresponding label names into labelNames, the data into X, and the names of the features into featureNames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import some data to play with\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "# Store the labels (y), label names, features (X), and feature names\n",
    "y = iris.target       # Labels are stored in y as numbers\n",
    "labelNames = iris.target_names # Species names corresponding to labels 0, 1, and 2\n",
    "X = iris.data\n",
    "featureNames = iris.feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we plot the first two features from the dataset (sepal length and width). Normally we would try to use all useful features, but sticking with two allows us to visualize the data more easily.\n",
    "\n",
    "Then we plot the data to get a look at what we're dealing with. The colormap is used to determine what colors are used for each class when plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data\n",
    "\n",
    "# Sepal length and width\n",
    "X_small = X[:,:2]\n",
    "# Get the minimum and maximum values with an additional 0.5 border\n",
    "x_min, x_max = X_small[:, 0].min() - .5, X_small[:, 0].max() + .5\n",
    "y_min, y_max = X_small[:, 1].min() - .5, X_small[:, 1].max() + .5\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Plot the training points\n",
    "plt.scatter(X_small[:, 0], X_small[:, 1], c=y, cmap=cmap_bold)\n",
    "plt.xlabel('Sepal length (cm)')\n",
    "plt.ylabel('Sepal width (cm)')\n",
    "plt.title('Sepal width vs length')\n",
    "\n",
    "# Set the plot limits\n",
    "plt.xlim(x_min, x_max)\n",
    "plt.ylim(y_min, y_max)\n",
    "\n",
    "# Plot the legend\n",
    "plt.legend(labelList, labelNames)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlabeled data\n",
    "===\n",
    "K-means is an unsupervised learning method, which means it doesn't make use of data labels. This is useful when we're exploring a new dataset. We may not have labels for this dataset, but we want to see how it is grouped together and what examples are most similar to each other. Below we plot the data again, but this time without any labels. This is what k-means \"sees\" when we use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data\n",
    "\n",
    "# Sepal length and width\n",
    "X_small = X[:,:2]\n",
    "# Get the minimum and maximum values with an additional 0.5 border\n",
    "x_min, x_max = X_small[:, 0].min() - .5, X_small[:, 0].max() + .5\n",
    "y_min, y_max = X_small[:, 1].min() - .5, X_small[:, 1].max() + .5\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Plot the training points\n",
    "plt.scatter(X_small[:, 0], X_small[:, 1])\n",
    "plt.xlabel('Sepal length (cm)')\n",
    "plt.ylabel('Sepal width (cm)')\n",
    "plt.title('Sepal width vs length')\n",
    "\n",
    "# Set the plot limits\n",
    "plt.xlim(x_min, x_max)\n",
    "plt.ylim(y_min, y_max)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-means: training\n",
    "===\n",
    "Next, we train a K-means classifier on our data. \n",
    "\n",
    "The first section chooses the number of clusters to use, and stores it in the variable n_clusters. We choose 3 because we know there are 3 species of iris, but we don't always know this when approaching a machine learning problem. \n",
    "\n",
    "The last two lines create and train the classifier. \n",
    "\n",
    "The first line creates a classifier (kmeans) using the [KMeans()](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) function, and tells it to use the number of neighbors stored in n_neighbors. The second line uses the fit() method to train the classifier on the features in X. Notice that because this is an unsupervised method, it does not use the labels stored in y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose your number of clusters\n",
    "n_clusters = 3\n",
    "\n",
    "# we create an instance of KMeans Classifier and fit the data.\n",
    "kmeans = KMeans(n_clusters=n_clusters)\n",
    "kmeans.fit(X_small)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the classification boundaries\n",
    "===\n",
    "Now that we have our classifier, let's visualize what it's doing. \n",
    "\n",
    "First we plot the decision boundaries, or the lines dividing areas assigned to the different clusters. The background shows the areas that are considered to belong to a certain cluster, and each cluster can then be assigned to a species of iris. They are plotted in grey, because the classifier does not assign labels to the clusters. The center of each cluster is plotted as a black x. Then we plot our examples onto the space, showing where each point lies in relation to the decision boundaries.\n",
    "\n",
    "If we took sepal measurements from a new flower, we could plot it in this space and use the background shade to determine which cluster of data points our classifier would assign to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = .02  # step size in the mesh\n",
    "\n",
    "# Plot the decision boundary. For that, we will assign a color to each\n",
    "# point in the mesh [x_min, m_max]x[y_min, y_max].\n",
    "x_min, x_max = X_small[:, 0].min() - 1, X_small[:, 0].max() + 1\n",
    "y_min, y_max = X_small[:, 1].min() - 1, X_small[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "\n",
    "Z = kmeans.predict(np.c_[xx.ravel(), yy.ravel()]) # Make a prediction oat every point \n",
    "                                               # in the mesh in order to find the \n",
    "                                               # classification areas for each label\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.pcolormesh(xx, yy, Z, cmap=cmap_bg)\n",
    "\n",
    "# Plot the training points\n",
    "plt.scatter(X_small[:, 0], X_small[:, 1])\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max())\n",
    "plt.title(\"KMeans (k = %i)\"\n",
    "         % (n_clusters))\n",
    "plt.xlabel('Sepal length (cm)')\n",
    "plt.ylabel('Sepal width (cm)')\n",
    "\n",
    "# Plot the centroids as a black X\n",
    "centroids = kmeans.cluster_centers_\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1],\n",
    "           marker='x', s=169, linewidths=3,\n",
    "           color='k', zorder=10)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cheating with labels\n",
    "---\n",
    "Because we do have labels for this dataset, let's see how well k-means did at separating the three species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = .02  # step size in the mesh\n",
    "\n",
    "# Plot the decision boundary. For that, we will assign a color to each\n",
    "# point in the mesh [x_min, m_max]x[y_min, y_max].\n",
    "x_min, x_max = X_small[:, 0].min() - 1, X_small[:, 0].max() + 1\n",
    "y_min, y_max = X_small[:, 1].min() - 1, X_small[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "\n",
    "Z = kmeans.predict(np.c_[xx.ravel(), yy.ravel()]) # Make a prediction oat every point \n",
    "                                               # in the mesh in order to find the \n",
    "                                               # classification areas for each label\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.pcolormesh(xx, yy, Z, cmap=cmap_bg)\n",
    "\n",
    "# Plot the training points\n",
    "plt.scatter(X_small[:, 0], X_small[:, 1], c=y, cmap=cmap_bold)\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max())\n",
    "plt.title(\"KMeans (k = %i)\"\n",
    "         % (n_clusters))\n",
    "plt.xlabel('Sepal length (cm)')\n",
    "plt.ylabel('Sepal width (cm)')\n",
    "\n",
    "# Plot the centroids as a black X\n",
    "centroids = kmeans.cluster_centers_\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1],\n",
    "           marker='x', s=169, linewidths=3,\n",
    "           color='k', zorder=10)\n",
    "\n",
    "# Plot the legend\n",
    "plt.legend(labelList, labelNames)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyzing the clusters\n",
    "===\n",
    "\n",
    "As you can see in the previous plots, K-means does a good job of separating the Setosa species (red) into its own cluster. It also does a reasonable job separating Versicolour (green) and Virginica (blue), although there is a considerable amount of overlap that it can't predict properly.\n",
    "\n",
    "This is an example where it is important to understand your data (and visualize it whenever possible), as well as understand your machine learning model. In this example, you may want to use a different machine learning model that can separate the data more accurately. Alternatively, we could use all four features to see if that improves accuracy (remember, we aren't using petal length or width here for easier data visualization).\n",
    "\n",
    "Changing the number of clusters\n",
    "---\n",
    "What would happen if you changed the number of clusters? What would the plot look like with 2 clusters, or 5? Based on the unlabeled data, how would you try to determine the number of classes to use?\n",
    "\n",
    "In the next block of code, try changing the number of clusteres and seeing what happens. You may need to change the number of colors represented in cmap_bg to match the number of classes you are using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code goes here!\n",
    "#cmap_bg = ListedColormap(['#111111','#333333', '#555555', '#777777', '#999999'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making Predictions\n",
    "===\n",
    "\n",
    "Now, let's say we go out and measure the sepals of two iris plants, and want to know what group they belong to. We're going to use our classifier to predict the flowers with the following measurements:\n",
    "\n",
    "Plant | Sepal length | Sepal width\n",
    "------|--------------|------------\n",
    "A     |4.3           |2.5\n",
    "B     |6.3           |2.1\n",
    "\n",
    "We can use our classifier's [predict()](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans.predict) function to predict the label for our input features. We pass in the variable examples to the predict() function, which is a list, and each element is another list containing the features (measurements) for a particular example. The output is a list of labels corresponding to the input examples.\n",
    "\n",
    "We'll also plot them on the boundary plot, to show why they were predicted that way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add our new data examples\n",
    "examples = [[4.3, 2.5], # Plant A\n",
    "            [6.3, 2.1]] # Plant B\n",
    "\n",
    "# Choose your number of clusters\n",
    "n_clusters = 3\n",
    "\n",
    "# we create an instance of KMeans Classifier and fit the data.\n",
    "kmeans = KMeans(n_clusters=n_clusters)\n",
    "kmeans.fit(X_small)\n",
    "\n",
    "# Predict the labels for our new examples\n",
    "labels = kmeans.predict(examples)\n",
    "\n",
    "# Print the predicted species names\n",
    "print('A: Cluster ' + str(labels[0]))\n",
    "print('B: Cluster ' + str(labels[1]))\n",
    "\n",
    "# Now plot the results\n",
    "h = .02  # step size in the mesh\n",
    "\n",
    "# Plot the decision boundary. For that, we will assign a color to each\n",
    "# point in the mesh [x_min, m_max]x[y_min, y_max].\n",
    "x_min, x_max = X_small[:, 0].min() - 1, X_small[:, 0].max() + 1\n",
    "y_min, y_max = X_small[:, 1].min() - 1, X_small[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "Z = kmeans.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.pcolormesh(xx, yy, Z, cmap=cmap_bg)\n",
    "\n",
    "# Plot the training points\n",
    "plt.scatter(X_small[:, 0], X_small[:, 1])\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max())\n",
    "plt.title(\"KMeans (k = %i)\"\n",
    "         % (n_clusters))\n",
    "plt.xlabel('Sepal length (cm)')\n",
    "plt.ylabel('Sepal width (cm)')\n",
    "\n",
    "# Plot the centroids as a black X\n",
    "centroids = kmeans.cluster_centers_\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1],\n",
    "           marker='x', s=169, linewidths=3,\n",
    "           color='k', zorder=10)\n",
    "\n",
    "# Display the new examples as labeled text on the graph\n",
    "plt.text(examples[0][0], examples[0][1],'A', fontsize=14)\n",
    "plt.text(examples[1][0], examples[1][1],'B', fontsize=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, example A is grouped into Cluster 2 and example B is grouped into Cluster 0. Remember, K-means does not use labels. It only clusters the data by feature similarity, and it's up to us to decide what the clusters mean (or if they don't mean anything at all).\n",
    "\n",
    "Using different features\n",
    "===\n",
    "Try using different combinations of the four features and see what results you get. Does it make it any easier to determine how many clusters should be used?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code goes here!\n",
    "#cmap_bg = ListedColormap(['#111111','#333333', '#555555', '#777777', '#999999'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final Notes\n",
    "===\n",
    "\n",
    "Some final things to keep in mind when using K-means to cluster your data\n",
    "\n",
    "* K-means is unsupervised, meaning it clusters data by similarity of features and does not require (or even use) labels.\n",
    "* How well it works is partially dependent on choosing the right number of clusters for the dataset. You can do this using your knowledge of the data (like we did, knowing we are looking at 3 species of plant). Alternatively, there are ways to try to experimentally find the best number of clusters.\n",
    "* The output does not provide a meaningful label, only a cluster assignment for the data. It is up to you to determine the meaning of each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
